{
  
    
        "post0": {
            "title": "BirdCLEF - Genkend fugle baseret på deres sang og kald",
            "content": "!pip install -Uqq fastbook import fastbook #fastbook.setup_book() . |████████████████████████████████| 727kB 25.3MB/s |████████████████████████████████| 204kB 44.0MB/s |████████████████████████████████| 1.2MB 37.1MB/s |████████████████████████████████| 51kB 8.1MB/s |████████████████████████████████| 61kB 9.3MB/s |████████████████████████████████| 61kB 9.7MB/s . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . #!unzip -u &quot;/content/drive/My Drive/Deep/TrainImages.zip&quot; -d &quot;/content/drive/My Drive/Deep/Birdsounds&quot; #!unzip -u &quot;/content/drive/My Drive/Deep/Bird500.zip&quot; -d &quot;/content/drive/My Drive/Deep/Birdsounds/train_short_audio&quot; . import pandas as pd import numpy as np import librosa import librosa.display import IPython.display as ipd import matplotlib.pyplot as plt import shutil, math, os from fastai.vision.all import * . root = &#39;/content/drive/My Drive/Deep/&#39; audio_path = root + &#39;Birdsounds/train_short_audio/Bird500/&#39; image_path = root + &#39;Images/&#39; sample_path = image_path +&#39;sample_images&#39; . meta_df = pd.read_csv(root + &#39;train_metadata.csv&#39;) meta_df.head() . primary_label secondary_labels type latitude longitude scientific_name common_name author date filename license rating time url . 0 acafly | [&#39;amegfi&#39;] | [&#39;begging call&#39;, &#39;call&#39;, &#39;juvenile&#39;] | 35.3860 | -84.1250 | Empidonax virescens | Acadian Flycatcher | Mike Nelson | 2012-08-12 | XC109605.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 2.5 | 09:30 | https://www.xeno-canto.org/109605 | . 1 acafly | [] | [&#39;call&#39;] | 9.1334 | -79.6501 | Empidonax virescens | Acadian Flycatcher | Allen T. Chartier | 2000-12-26 | XC11209.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 3.0 | ? | https://www.xeno-canto.org/11209 | . 2 acafly | [] | [&#39;call&#39;] | 5.7813 | -75.7452 | Empidonax virescens | Acadian Flycatcher | Sergio Chaparro-Herrera | 2012-01-10 | XC127032.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 3.0 | 15:20 | https://www.xeno-canto.org/127032 | . 3 acafly | [&#39;whwbec1&#39;] | [&#39;call&#39;] | 4.6717 | -75.6283 | Empidonax virescens | Acadian Flycatcher | Oscar Humberto Marin-Gomez | 2009-06-19 | XC129974.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 3.5 | 07:50 | https://www.xeno-canto.org/129974 | . 4 acafly | [&#39;whwbec1&#39;] | [&#39;call&#39;] | 4.6717 | -75.6283 | Empidonax virescens | Acadian Flycatcher | Oscar Humberto Marin-Gomez | 2009-06-19 | XC129981.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 3.5 | 07:50 | https://www.xeno-canto.org/129981 | . Dataen til dette projekt kommer fra en konkurrence på kaggle. Dataen blev udleveret som lydfiler med optagelser af en til flere fugle og med en csv fil med metadata som blandt andet inderholder den primære fugl og sekundærer fugle der også fremgår i klippet. . Da der er alt for mange forskellige fugle til at det kan gøres på en ordentlig måde har jeg valgt at reducerer antallet af labels til 12 fugle, hvilket er de 12 hvor af der var flest optagelser. . For at der kan laves en klassifikation på lydklippene skal de først omdannes til billeder. For at gøre dette plottede jeg dem som deres spektogrammer med et python modul der hedder librosa. De plots gemmes så som png filer. . for folder in os.listdir(audio_path): print(folder) for recording in os.listdir(audio_path + folder): filepath = audio_path + folder + &quot;/&quot; + recording filename = recording.split(&#39;.&#39;)[0] imagename = root + f&#39;Images/{filename}.png&#39; if os.path.exists(imagename) : continue sig, rate = librosa.load(filepath, sr=32000, offset=0, duration=10) # First, compute the spectrogram using the &quot;short-time Fourier transform&quot; (stft) spec = librosa.stft(sig) # Scale the amplitudes according to the decibel scale spec_db = librosa.amplitude_to_db(spec, ref=np.max) # Plot the spectrogram plt.figure(figsize=(15, 5)) librosa.display.specshow(spec_db, sr=32000, x_axis=&#39;time&#39;, y_axis=&#39;hz&#39;, cmap=plt.get_cmap(&#39;viridis&#39;)) plt.savefig(imagename) plt.clf() plt.close(&#39;all&#39;) . Det resulterede i over 5000 billeder hvilket gjorde træningen utroligt langsom, så for at gøre processen en anelse hurtigere lavede jeg er et tilfældigt trænings sæt af 50 billeder pr fugl for også at holde det balanceret. . Dernæst sikre jeg mig en liste over fugle der er billed filer af for at de labels kan bruges senere. . converted_images = [i.split(&#39;.&#39;)[0]+&#39;.ogg&#39; for i in os.listdir(image_path)] converted_images_df = meta_df[meta_df.filename.isin(converted_images)] converted_labels = converted_images_df.primary_label.values[0] . Jeg skal bruge en metode til at få fat i y værdierne, fuglens navn, for at kunne lære hvilket kald kommer fra hvilken fugl. Det er i meta data filen og kan trækkes herfra. De sekundære labels er dog ikke en liste men desværre en string der ligner en liste. Strengen kan heldigvis let skilles ad. . def get_bird_label(file): file = str(file.parts[-1]) file = file.split(&#39; &#39;)[0] filename = file.split(&#39;.&#39;)[0] + &#39;.ogg&#39; row = meta_df[meta_df[&#39;filename&#39;] == filename] bird_label = [row[&#39;primary_label&#39;].values[0]] secondaries = row.secondary_labels.values[0] secondaries.replace(&#39;[&#39;, &#39;&#39;) secondaries.replace(&#39;]&#39;, &#39;&#39;) for label in secondaries.split(&#39;,&#39;): if label not in converted_labels: continue bird_label.append(label) print(bird_label) return bird_label . Jeg bruger så en DataBlock til at samle metoder for at behandle og transformere dataen til når den skal bruges. . birds = DataBlock( blocks=(ImageBlock, MultiCategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=get_bird_label, item_tfms=RandomResizedCrop(128)) . Jeg omdanner den så til en dataloader og giver den stigen til min data. . dls = birds.dataloaders(image_path) . Da det er her er planen at bruge billeder til at klassificere lyd giver det bedst mening at bruge et Convolutional Neural Network (CNN) da disse er de bedste til analyse af billeder. Det gør de ved at trække features ud af billeder gennem et utal af filtre. Hvert enkelt filter udtrækker hver især en bid af information, som bliver mere og mere specifikke for hvert lag. De tidligste lag finder ting som lodrette og vandrette linjer, nogle af dem i midten finder ting som hjørner og cirkler, mens de sidste lag kan finde ting som kan gå fra kropsdele helt op til hele objekter. . Hertil bruges resnet18 som er et residualt net med 18 skjulte lag. Det er en pretrained model som bruges her til transfer learning. Transfer Learning er en måde hvorpå man kan reducere tiden det tager at træne sin model, og reducere mængden af data i forhold til hvad man før har behøvet. Det kan man da modellen i forvejen har lært sig at udtrække de basale former og mønstre og derfor kun skal tilpasses til den nye data. Den gamle model kaldes for kroppen, og den nye model bliver til et nyt hoved til kroppen. . learn = cnn_learner(dls, resnet18, metrics=partial(accuracy_multi, thresh=0.02)).to_fp16() . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . Da vi nu bruger transfer learning skal vi tilpasse vores krop og hoved til hinanden. Der er to forskellige metoder til at gøre dette: Fit og Fine Tune. Fit tager kroppen og prøver at træne videre og tilpasse både kroppen og hovedet til at arbejde med den nye data. Dette kan desværre ødelægge alt hvad der har været gjort på forhånd i den trænede model. Fine Tune på den anden side tilpasser kun lidt og træner kun hovedet der derfor ikke generer kroppen. . For at forbedre både træningstiden og resultatet deraf gør jeg brug af learning rate finder. Den løber igennem dataen og finder en funktion der beskriver udviklingen for modellens loss. Den outputter så to værdier, der hvor loss ændrer sig mest drastisk og der hvor loss ændrer sig mindst. Jeg bruger så der hvor den ændrer sig mest som min learning rate. . lr_min, lr_steep = learn.lr_find() learn.fine_tune(5, base_lr=lr_steep) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.455330 | 0.328733 | 0.437266 | 10:19 | . . 80.00% [4/5 57:02&lt;14:15] epoch train_loss valid_loss accuracy_multi time . 0 | 0.252473 | 0.282650 | 0.539995 | 14:33 | . 1 | 0.223416 | 0.254896 | 0.550762 | 14:07 | . 2 | 0.193182 | 0.221853 | 0.489299 | 14:01 | . 3 | 0.161605 | 0.208035 | 0.596241 | 14:20 | . . 10.39% [8/77 00:48&lt;06:55 0.1603] &lt;/div&gt; &lt;/div&gt; epoch train_loss valid_loss accuracy_multi time . 0 | 0.252473 | 0.282650 | 0.539995 | 14:33 | . 1 | 0.223416 | 0.254896 | 0.550762 | 14:07 | . 2 | 0.193182 | 0.221853 | 0.489299 | 14:01 | . 3 | 0.161605 | 0.208035 | 0.596241 | 14:20 | . 4 | 0.145195 | 0.197106 | 0.643593 | 14:23 | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.fine_tune(5) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.136180 | 0.196756 | 0.630284 | 14:14 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.133849 | 0.197100 | 0.636437 | 14:35 | . 1 | 0.132857 | 0.193359 | 0.656501 | 13:55 | . 2 | 0.128305 | 0.197004 | 0.662319 | 13:44 | . 3 | 0.122739 | 0.190228 | 0.650950 | 14:01 | . 4 | 0.121430 | 0.191938 | 0.664660 | 13:41 | . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_top_losses(10, nrows=2) . target predicted probabilities loss . 0 norcar | spotow | tensor([1.8487e-05, 5.1337e-06, 2.4346e-04, 1.0534e-05, 9.9729e-06, 4.6839e-05, 4.4338e-03, 3.2758e-04, 1.3420e-05, 2.7362e-03, 1.5870e-07, 9.9136e-01]) | 1.0651806592941284 | . 1 houspa | redcro | tensor([1.7131e-04, 1.4064e-05, 1.7537e-04, 9.2958e-06, 9.9930e-05, 7.0141e-04, 1.0403e-03, 2.5714e-04, 9.9472e-01, 1.6941e-03, 4.7950e-05, 1.7953e-04]) | 1.0424740314483643 | . 2 houspa | redcro | tensor([6.1904e-04, 1.5720e-04, 5.4421e-04, 1.9564e-04, 1.5968e-04, 2.9009e-03, 2.5410e-03, 2.0348e-03, 9.6021e-01, 3.6288e-02, 1.2944e-03, 1.0005e-03]) | 0.7593716979026794 | . 3 spotow | eursta | tensor([0.0318, 0.0013, 0.0057, 0.8055, 0.0094, 0.0180, 0.0782, 0.0011, 0.0056, 0.0036, 0.0014, 0.0012]) | 0.7120441794395447 | . 4 houwre | eursta | tensor([2.2890e-02, 8.1993e-04, 2.3735e-02, 9.1993e-01, 1.7007e-03, 3.7362e-03, 2.6626e-03, 5.6148e-04, 5.9111e-03, 1.0902e-02, 6.0504e-03, 2.2693e-04]) | 0.7108751535415649 | . 5 houwre | gbwwre1 | tensor([9.3264e-04, 9.6221e-04, 1.4494e-03, 8.0620e-03, 9.7863e-01, 2.1319e-04, 1.0328e-02, 6.9324e-04, 1.5189e-03, 1.2304e-03, 4.2982e-05, 6.3124e-04]) | 0.7028728723526001 | . 6 norcar | houwre | tensor([2.3141e-03, 7.6430e-04, 1.0530e-02, 7.6362e-03, 1.2194e-02, 3.3244e-03, 8.9859e-01, 2.4534e-03, 2.5910e-03, 8.1082e-02, 7.6025e-05, 2.9810e-03]) | 0.7021654844284058 | . 7 houspa | redcro | tensor([1.5978e-03, 2.3597e-04, 9.9661e-04, 4.1407e-04, 9.3994e-04, 5.1396e-03, 1.1961e-02, 1.6679e-03, 9.5150e-01, 2.4191e-02, 5.7479e-04, 1.2113e-03]) | 0.6951014399528503 | . 8 spotow | eursta | tensor([0.2765, 0.0027, 0.0263, 0.6921, 0.0070, 0.0041, 0.0085, 0.0019, 0.0230, 0.0129, 0.0087, 0.0016]) | 0.6703795194625854 | . 9 spotow | eursta | tensor([0.0893, 0.0039, 0.1854, 0.5698, 0.0269, 0.0072, 0.0113, 0.0020, 0.0129, 0.0087, 0.0032, 0.0011]) | 0.667083203792572 | . Modellen har desværre klaret sig så dårligt at det ville give meget lidt værdi at prøve at bruge den til at lave predictions, da det kun ville være lidt bedre end at gætte sig frem selv. Det er ikke at sige at den ikke ville kunne forbedres til at punkt hvor den ville kunne bruges i produktion. Eksempelvist ville en bedre loss function potentielt kunne give bedre resultater, eller tilsvarende for en bedre eller mere specifik learner. Der er desværre bare gået for meget tid med at optimere på modeller og data behandling til at disse muligheder har haft deres chance. . &lt;/div&gt; .",
            "url": "https://markbringnielsen.github.io/BirdsBlog/2021/06/08/_05_25_Birds.html",
            "relUrl": "/2021/06/08/_05_25_Birds.html",
            "date": " • Jun 8, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "!pip install -Uqq fastbook import fastbook #fastbook.setup_book() . |████████████████████████████████| 727kB 7.7MB/s |████████████████████████████████| 51kB 8.4MB/s |████████████████████████████████| 204kB 18.7MB/s |████████████████████████████████| 1.2MB 18.9MB/s |████████████████████████████████| 61kB 9.5MB/s |████████████████████████████████| 51kB 8.3MB/s . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . #!unzip -u &quot;/content/drive/My Drive/Deep/TrainImages.zip&quot; -d &quot;/content/drive/My Drive/Deep/Birdsounds&quot; !unzip -u &quot;/content/drive/My Drive/Deep/Bird500.zip&quot; -d &quot;/content/drive/My Drive/Deep/Birdsounds/train_short_audio&quot; . Archive: /content/drive/My Drive/Deep/Bird500.zip warning [/content/drive/My Drive/Deep/Bird500.zip]: 45317897 extra bytes at beginning or within zipfile (attempting to process anyway) error [/content/drive/My Drive/Deep/Bird500.zip]: start of central directory not found; zipfile corrupt. (please check that you have transferred or created the zipfile in the appropriate BINARY mode and that you have compiled UnZip properly) . import pandas as pd import numpy as np import librosa import librosa.display import IPython.display as ipd import matplotlib.pyplot as plt import shutil, math, os . root = &#39;/content/drive/My Drive/Deep/&#39; audio_path = root + &#39;Birdsounds/train_short_audio/&#39; test_path = audio_path + &#39;XC1619.ogg&#39; image_path = root + &#39;Images/&#39; . ipd.Audio(test_path) . Your browser does not support the audio element. librosa.get_duration(filename=test_path) . 82.11703125 . for recording in os.listdir(audio_path): filepath = audio_path + recording filename = recording.split(&#39;.&#39;)[0] offset = 0 duration = 10 for i in range(math.floor(librosa.get_duration(filename=filepath)/duration)): imagename = root + f&#39;Images/{filename}-{offset}-{offset + duration}.png&#39; if os.path.exists(imagename) : continue sig, rate = librosa.load(filepath, sr=32000, offset=offset, duration=duration) # First, compute the spectrogram using the &quot;short-time Fourier transform&quot; (stft) spec = librosa.stft(sig) # Scale the amplitudes according to the decibel scale spec_db = librosa.amplitude_to_db(spec, ref=np.max) # Plot the spectrogram plt.figure(figsize=(15, 5)) librosa.display.specshow(spec_db, sr=32000, x_axis=&#39;time&#39;, y_axis=&#39;hz&#39;, cmap=plt.get_cmap(&#39;viridis&#39;)) plt.savefig(imagename) plt.clf() plt.close(&#39;all&#39;) offset += duration . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-8-cc12a94c0c69&gt; in &lt;module&gt;() 4 offset = 0 5 duration = 10 -&gt; 6 for i in range(math.floor(librosa.get_duration(filename=filepath)/duration)): 7 imagename = root + f&#39;Images/{filename}-{offset}-{offset + duration}.png&#39; 8 if os.path.exists(imagename) : continue /usr/local/lib/python3.7/dist-packages/librosa/core/audio.py in get_duration(y, sr, S, n_fft, hop_length, center, filename) 677 if filename is not None: 678 try: --&gt; 679 return sf.info(filename).duration 680 except RuntimeError: 681 with audioread.audio_open(filename) as fdesc: /usr/local/lib/python3.7/dist-packages/soundfile.py in info(file, verbose) 436 Whether to print additional information. 437 &#34;&#34;&#34; --&gt; 438 return _SoundFileInfo(file, verbose) 439 440 /usr/local/lib/python3.7/dist-packages/soundfile.py in __init__(self, file, verbose) 381 def __init__(self, file, verbose): 382 self.verbose = verbose --&gt; 383 with SoundFile(file) as f: 384 self.name = f.name 385 self.samplerate = f.samplerate /usr/local/lib/python3.7/dist-packages/soundfile.py in __init__(self, file, mode, samplerate, channels, subtype, endian, format, closefd) 627 self._info = _create_info_struct(file, mode, samplerate, channels, 628 format, subtype, endian) --&gt; 629 self._file = self._open(file, mode_int, closefd) 630 if set(mode).issuperset(&#39;r+&#39;) and self.seekable(): 631 # Move write position to 0 (like in Python file objects) /usr/local/lib/python3.7/dist-packages/soundfile.py in _open(self, file, mode_int, closefd) 1173 else: 1174 file = file.encode(_sys.getfilesystemencoding()) -&gt; 1175 file_ptr = openfunction(file, mode_int, self._info) 1176 elif isinstance(file, int): 1177 file_ptr = _snd.sf_open_fd(file, mode_int, self._info, closefd) KeyboardInterrupt: . meta_df = pd.read_csv(root+&#39;train_metadata.csv&#39;) bird500 = meta_df[meta_df[&#39;primary_label&#39;].map(meta_df[&#39;primary_label&#39;].value_counts()) == 500] bird500_gb = bird500.groupby(&#39;primary_label&#39;).apply(lambda x: x.sample(100, random_state=42)) offset = 2 duration = 10 for i, row in bird500_gb.iterrows(): bird = row.primary_label file = row.filename filepath = audio_path + f&#39;{bird}/{file}&#39; filename = file.split(&#39;.&#39;)[0] imagename = root + f&#39;TrainImages/{filename}.png&#39; sig, rate = librosa.load(filepath, sr=32000, offset=offset, duration=duration) # First, compute the spectrogram using the &quot;short-time Fourier transform&quot; (stft) spec = librosa.stft(sig) # Scale the amplitudes according to the decibel scale spec_db = librosa.amplitude_to_db(spec, ref=np.max) # Plot the spectrogram plt.figure(figsize=(15, 5)) librosa.display.specshow(spec_db, sr=32000, x_axis=&#39;time&#39;, y_axis=&#39;hz&#39;, cmap=plt.get_cmap(&#39;viridis&#39;)) plt.savefig(imagename) plt.clf() plt.close(&#39;all&#39;) . /usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:162: UserWarning: PySoundFile failed. Trying audioread instead. warnings.warn(&#34;PySoundFile failed. Trying audioread instead.&#34;) . RuntimeError Traceback (most recent call last) /usr/local/lib/python3.7/dist-packages/librosa/core/audio.py in load(path, sr, mono, offset, duration, dtype, res_type) 145 try: --&gt; 146 with sf.SoundFile(path) as sf_desc: 147 sr_native = sf_desc.samplerate /usr/local/lib/python3.7/dist-packages/soundfile.py in __init__(self, file, mode, samplerate, channels, subtype, endian, format, closefd) 628 format, subtype, endian) --&gt; 629 self._file = self._open(file, mode_int, closefd) 630 if set(mode).issuperset(&#39;r+&#39;) and self.seekable(): /usr/local/lib/python3.7/dist-packages/soundfile.py in _open(self, file, mode_int, closefd) 1183 _error_check(_snd.sf_error(file_ptr), -&gt; 1184 &#34;Error opening {0!r}: &#34;.format(self.name)) 1185 if mode_int == _snd.SFM_WRITE: /usr/local/lib/python3.7/dist-packages/soundfile.py in _error_check(err, prefix) 1356 err_str = _snd.sf_error_number(err) -&gt; 1357 raise RuntimeError(prefix + _ffi.string(err_str).decode(&#39;utf-8&#39;, &#39;replace&#39;)) 1358 RuntimeError: Error opening &#39;/content/drive/My Drive/Deep/Birdsounds/train_short_audio/barswa/XC484700.ogg&#39;: System error. During handling of the above exception, another exception occurred: FileNotFoundError Traceback (most recent call last) &lt;ipython-input-12-0d8979d2f23c&gt; in &lt;module&gt;() 14 15 imagename = root + f&#39;TrainImages/{filename}.png&#39; &gt; 16 sig, rate = librosa.load(filepath, sr=32000, offset=offset, duration=duration) 17 18 # First, compute the spectrogram using the &#34;short-time Fourier transform&#34; (stft) /usr/local/lib/python3.7/dist-packages/librosa/core/audio.py in load(path, sr, mono, offset, duration, dtype, res_type) 161 if isinstance(path, (str, pathlib.PurePath)): 162 warnings.warn(&#34;PySoundFile failed. Trying audioread instead.&#34;) --&gt; 163 y, sr_native = __audioread_load(path, offset, duration, dtype) 164 else: 165 raise (exc) /usr/local/lib/python3.7/dist-packages/librosa/core/audio.py in __audioread_load(path, offset, duration, dtype) 185 186 y = [] --&gt; 187 with audioread.audio_open(path) as input_file: 188 sr_native = input_file.samplerate 189 n_channels = input_file.channels /usr/local/lib/python3.7/dist-packages/audioread/__init__.py in audio_open(path, backends) 109 for BackendClass in backends: 110 try: --&gt; 111 return BackendClass(path) 112 except DecodeError: 113 pass /usr/local/lib/python3.7/dist-packages/audioread/rawread.py in __init__(self, filename) 60 &#34;&#34;&#34; 61 def __init__(self, filename): &gt; 62 self._fh = open(filename, &#39;rb&#39;) 63 64 try: FileNotFoundError: [Errno 2] No such file or directory: &#39;/content/drive/My Drive/Deep/Birdsounds/train_short_audio/barswa/XC484700.ogg&#39; . import os len(os.listdir(&#39;/content/drive/My Drive/Deep/Birdsounds/TrainImages&#39;)) . 241 . from fastai.vision.all import * . meta_df = pd.read_csv(root + &#39;train_metadata.csv&#39;) meta_df.head() . primary_label secondary_labels type latitude longitude scientific_name common_name author date filename license rating time url . 0 acafly | [&#39;amegfi&#39;] | [&#39;begging call&#39;, &#39;call&#39;, &#39;juvenile&#39;] | 35.3860 | -84.1250 | Empidonax virescens | Acadian Flycatcher | Mike Nelson | 2012-08-12 | XC109605.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 2.5 | 09:30 | https://www.xeno-canto.org/109605 | . 1 acafly | [] | [&#39;call&#39;] | 9.1334 | -79.6501 | Empidonax virescens | Acadian Flycatcher | Allen T. Chartier | 2000-12-26 | XC11209.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 3.0 | ? | https://www.xeno-canto.org/11209 | . 2 acafly | [] | [&#39;call&#39;] | 5.7813 | -75.7452 | Empidonax virescens | Acadian Flycatcher | Sergio Chaparro-Herrera | 2012-01-10 | XC127032.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 3.0 | 15:20 | https://www.xeno-canto.org/127032 | . 3 acafly | [&#39;whwbec1&#39;] | [&#39;call&#39;] | 4.6717 | -75.6283 | Empidonax virescens | Acadian Flycatcher | Oscar Humberto Marin-Gomez | 2009-06-19 | XC129974.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 3.5 | 07:50 | https://www.xeno-canto.org/129974 | . 4 acafly | [&#39;whwbec1&#39;] | [&#39;call&#39;] | 4.6717 | -75.6283 | Empidonax virescens | Acadian Flycatcher | Oscar Humberto Marin-Gomez | 2009-06-19 | XC129981.ogg | Creative Commons Attribution-NonCommercial-ShareAlike 3.0 | 3.5 | 07:50 | https://www.xeno-canto.org/129981 | . def get_bird_label(file): file = str(file.parts[-1]) filename = file.split(&#39;-&#39;)[0] + &#39;.ogg&#39; bird_label = meta_df[meta_df[&#39;filename&#39;] == filename][&#39;primary_label&#39;].values[0] return bird_label . fns = get_image_files(image_path) fns . failed = verify_images(fns) failed . (#0) [] . failed.map(Path.unlink); . birds = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=get_bird_label, item_tfms=RandomResizedCrop(128)) . dls = birds.dataloaders(image_path) . learn = cnn_learner(dls, resnet152, metrics=error_rate).to_fp16() learn.fit_one_cycle(4) learn.unfreeze() learn.fit_one_cycle(4) . . 0.00% [0/4 00:00&lt;00:00] epoch train_loss valid_loss error_rate time . . 98.52% [266/270 1:06:19&lt;00:59 2.3173] &lt;/div&gt; &lt;/div&gt; OSError Traceback (most recent call last) &lt;ipython-input-10-a6fb24dab211&gt; in &lt;module&gt;() 1 learn = cnn_learner(dls, resnet152, metrics=error_rate) -&gt; 2 learn.fit_one_cycle(4) 3 learn.unfreeze() 4 learn.fit_one_cycle(4) /usr/local/lib/python3.7/dist-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt) 110 scheds = {&#39;lr&#39;: combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final), 111 &#39;mom&#39;: combined_cos(pct_start, *(self.moms if moms is None else moms))} --&gt; 112 self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd) 113 114 # Cell /usr/local/lib/python3.7/dist-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt) 216 self.opt.set_hypers(lr=self.lr if lr is None else lr) 217 self.n_epoch = n_epoch --&gt; 218 self._with_events(self._do_fit, &#39;fit&#39;, CancelFitException, self._end_cleanup) 219 220 def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 158 159 def _with_events(self, f, event_type, ex, final=noop): --&gt; 160 try: self(f&#39;before_{event_type}&#39;); f() 161 except ex: self(f&#39;after_cancel_{event_type}&#39;) 162 self(f&#39;after_{event_type}&#39;); final() /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _do_fit(self) 207 for epoch in range(self.n_epoch): 208 self.epoch=epoch --&gt; 209 self._with_events(self._do_epoch, &#39;epoch&#39;, CancelEpochException) 210 211 def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False): /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 158 159 def _with_events(self, f, event_type, ex, final=noop): --&gt; 160 try: self(f&#39;before_{event_type}&#39;); f() 161 except ex: self(f&#39;after_cancel_{event_type}&#39;) 162 self(f&#39;after_{event_type}&#39;); final() /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _do_epoch(self) 201 202 def _do_epoch(self): --&gt; 203 self._do_epoch_train() 204 self._do_epoch_validate() 205 /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _do_epoch_train(self) 193 def _do_epoch_train(self): 194 self.dl = self.dls.train --&gt; 195 self._with_events(self.all_batches, &#39;train&#39;, CancelTrainException) 196 197 def _do_epoch_validate(self, ds_idx=1, dl=None): /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 158 159 def _with_events(self, f, event_type, ex, final=noop): --&gt; 160 try: self(f&#39;before_{event_type}&#39;); f() 161 except ex: self(f&#39;after_cancel_{event_type}&#39;) 162 self(f&#39;after_{event_type}&#39;); final() /usr/local/lib/python3.7/dist-packages/fastai/learner.py in all_batches(self) 164 def all_batches(self): 165 self.n_iter = len(self.dl) --&gt; 166 for o in enumerate(self.dl): self.one_batch(*o) 167 168 def _do_one_batch(self): /usr/local/lib/python3.7/dist-packages/fastai/data/load.py in __iter__(self) 107 self.before_iter() 108 self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses) --&gt; 109 for b in _loaders[self.fake_l.num_workers==0](self.fake_l): 110 if self.device is not None: b = to_device(b, self.device) 111 yield self.after_batch(b) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py in __next__(self) 515 if self._sampler_iter is None: 516 self._reset() --&gt; 517 data = self._next_data() 518 self._num_yielded += 1 519 if self._dataset_kind == _DatasetKind.Iterable and /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py in _next_data(self) 1197 else: 1198 del self._task_info[idx] -&gt; 1199 return self._process_data(data) 1200 1201 def _try_put_index(self): /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py in _process_data(self, data) 1223 self._try_put_index() 1224 if isinstance(data, ExceptionWrapper): -&gt; 1225 data.reraise() 1226 return data 1227 /usr/local/lib/python3.7/dist-packages/torch/_utils.py in reraise(self) 427 # have message field 428 raise self.exc_type(message=msg) --&gt; 429 raise self.exc_type(msg) 430 431 OSError: Caught OSError in DataLoader worker process 0. Original Traceback (most recent call last): File &#34;/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py&#34;, line 202, in _worker_loop data = fetcher.fetch(index) File &#34;/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py&#34;, line 34, in fetch data = next(self.dataset_iter) File &#34;/usr/local/lib/python3.7/dist-packages/fastai/data/load.py&#34;, line 118, in create_batches yield from map(self.do_batch, self.chunkify(res)) File &#34;/usr/local/lib/python3.7/dist-packages/fastcore/basics.py&#34;, line 216, in chunked res = list(itertools.islice(it, chunk_sz)) File &#34;/usr/local/lib/python3.7/dist-packages/fastai/data/load.py&#34;, line 133, in do_item try: return self.after_item(self.create_item(s)) File &#34;/usr/local/lib/python3.7/dist-packages/fastai/data/load.py&#34;, line 140, in create_item if self.indexed: return self.dataset[s or 0] File &#34;/usr/local/lib/python3.7/dist-packages/fastai/data/core.py&#34;, line 333, in __getitem__ res = tuple([tl[it] for tl in self.tls]) File &#34;/usr/local/lib/python3.7/dist-packages/fastai/data/core.py&#34;, line 333, in &lt;listcomp&gt; res = tuple([tl[it] for tl in self.tls]) File &#34;/usr/local/lib/python3.7/dist-packages/fastai/data/core.py&#34;, line 299, in __getitem__ return self._after_item(res) if is_indexer(idx) else res.map(self._after_item) File &#34;/usr/local/lib/python3.7/dist-packages/fastai/data/core.py&#34;, line 261, in _after_item def _after_item(self, o): return self.tfms(o) File &#34;/usr/local/lib/python3.7/dist-packages/fastcore/transform.py&#34;, line 200, in __call__ def __call__(self, o): return compose_tfms(o, tfms=self.fs, split_idx=self.split_idx) File &#34;/usr/local/lib/python3.7/dist-packages/fastcore/transform.py&#34;, line 150, in compose_tfms x = f(x, **kwargs) File &#34;/usr/local/lib/python3.7/dist-packages/fastcore/transform.py&#34;, line 73, in __call__ def __call__(self, x, **kwargs): return self._call(&#39;encodes&#39;, x, **kwargs) File &#34;/usr/local/lib/python3.7/dist-packages/fastcore/transform.py&#34;, line 83, in _call return self._do_call(getattr(self, fn), x, **kwargs) File &#34;/usr/local/lib/python3.7/dist-packages/fastcore/transform.py&#34;, line 89, in _do_call return retain_type(f(x, **kwargs), x, ret) File &#34;/usr/local/lib/python3.7/dist-packages/fastcore/dispatch.py&#34;, line 118, in __call__ return f(*args, **kwargs) File &#34;/usr/local/lib/python3.7/dist-packages/fastai/vision/core.py&#34;, line 110, in create return cls(load_image(fn, **merge(cls._open_args, kwargs))) File &#34;/usr/local/lib/python3.7/dist-packages/fastai/vision/core.py&#34;, line 86, in load_image im.load() File &#34;/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py&#34;, line 270, in load raise_ioerror(err_code) File &#34;/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py&#34;, line 59, in raise_ioerror raise OSError(message + &#34; when reading image file&#34;) OSError: unrecognized data stream contents when reading image file . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(5, nrows=1) . &lt;/div&gt; .",
            "url": "https://markbringnielsen.github.io/BirdsBlog/2021/05/25/Birds.html",
            "relUrl": "/2021/05/25/Birds.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://markbringnielsen.github.io/BirdsBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://markbringnielsen.github.io/BirdsBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://markbringnielsen.github.io/BirdsBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://markbringnielsen.github.io/BirdsBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}